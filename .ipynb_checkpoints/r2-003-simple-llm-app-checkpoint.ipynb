{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# How to build a simple LLM App with LangChain\n",
    "* Very simple LLM App: some prompting and a single LLM call.\n",
    "* Using LangChain v0.2 (Remember: LangChain v0.1 is still fully supported).\n",
    "* **As always, the LangChain documentation is not very easy to understand for beginners.** We will make several modifications to the original project from LangChain so you can understand the project easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b470b3-1008-44d5-bc0f-35140a27e3ab",
   "metadata": {},
   "source": [
    "## Goal of the App\n",
    "* Translate text from English into another language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0399355-dece-4701-9bf4-4c204fe74929",
   "metadata": {},
   "source": [
    "## Concepts included\n",
    "* Monitor with LangSmith.\n",
    "* Connect with a LLM.\n",
    "* Use a Prompt Template.\n",
    "* Use an Output Parser.\n",
    "* Chain the Prompt Template, the LLM call and the Output Parser.\n",
    "* Deploy with LangServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48386f20-c929-48a9-8720-0953fcd67dd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee060-21f2-4e01-b283-1fd656dac1e9",
   "metadata": {},
   "source": [
    "#### Recommended: create new virtualenv\n",
    "* mkdir your_project_name\n",
    "* cd your_project_name\n",
    "* pyenv virtualenv 3.11.4 your_venv_name\n",
    "* pyenv activate your_venv_name\n",
    "* pip install jupyterlab\n",
    "* jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f11855b-e12a-4756-8893-7d00244c8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2ba54-ace7-4719-b6c2-7713a615614d",
   "metadata": {},
   "source": [
    "#### .env File\n",
    "Remember to include:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a63ff6-99ff-4629-b965-547d12a99ba6",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **simpleTranslator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and ask it to translate  a sentence from English to Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e3790-7d10-4c72-81e6-d62895e34c2b",
   "metadata": {},
   "source": [
    "The `-qU` flag in the `pip install -qU langchain-openai` command is not necessary, although since the LangChain team seems to like it we will explain it here.\n",
    "\n",
    "It combines two separate options for the `pip` installer:\n",
    "\n",
    "1. `-q` (or `--quiet`): This option reduces the amount of output that `pip` generates during the installation process. Using `-q` reduces output to show only errors, and using it twice (`-qq`) will suppress output entirely, except for errors.\n",
    "\n",
    "2. `-U` (or `--upgrade`): This option tells `pip` to upgrade the specified package to the latest available version. It also upgrades its dependencies to the latest versions that are compatible with other installed packages.\n",
    "\n",
    "So, `pip install -qU langchain-openai` means \"install or upgrade the package `langchain-openai` quietly, showing minimal output unless there are errors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "#### OpenAI LLM options\n",
    "* See [OpenAI website](https://platform.openai.com/docs/models/gpt-3-5-turbo).\n",
    "* For this project, we will use gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1d409-7f2f-40a4-90c5-ad77dad3edce",
   "metadata": {},
   "source": [
    "* System Message: defines the role played by the LLM.\n",
    "* Human Message: the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5dc0613-fb6d-4c82-a614-9e7307714303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messagesToTheLLM = [\n",
    "    SystemMessage(content=\"Translate the following from English into Spanish\"),\n",
    "    HumanMessage(content=\"Generative AI is the greatest value-creation opportunity in Human History.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b36ec-341a-47bc-af32-30cfb939810d",
   "metadata": {},
   "source": [
    "#### Step 1: call the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4536f28-32f9-4ef8-9528-b07c37a57388",
   "metadata": {},
   "source": [
    "* `invoke` is the most basic way to call the LLM. We will see more ways of doing it soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db815e32-8e60-46b3-8cce-fbf40e378397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 32, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-39ae644f-4c3a-4c87-897d-34631695fb86-0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messagesToTheLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913fc8c-254f-410d-aa8f-35eb0898855e",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f836a-a1bd-4458-8972-204d1dfef5dd",
   "metadata": {},
   "source": [
    "## Use an Output Parser to format the response of the LLM as a string of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54fd7a7d-21ae-4131-bbce-8fba09482bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df9f80-b57c-4486-b5f6-0c28065a5ddf",
   "metadata": {},
   "source": [
    "#### Step 1: call the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c813944c-a65d-4b82-89b0-bcf034088493",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialResponse = llm.invoke(messagesToTheLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc331d-bf66-46fb-a527-dcbaab73d790",
   "metadata": {},
   "source": [
    "#### Step 2: apply the parser to format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dd7956e-4e5d-4ca8-b823-c508f2903883",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedResponse = parser.invoke(initialResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1077cf9-8e35-411e-9b57-89506c81d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.\n"
     ]
    }
   ],
   "source": [
    "print(formattedResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445f320-289d-4372-93eb-8e32b873b23c",
   "metadata": {},
   "source": [
    "## Chain the LLM Call and the Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07d2d4ed-11f7-41fa-ab1d-22fa4dcac792",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmAndParserChained = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b3692-a5d7-4550-8717-cb48a8232cd7",
   "metadata": {},
   "source": [
    "#### Steps 1 and 2: call the LLM and apply the parser to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da6a7652-268e-42a1-ad80-518a186c07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "responseFromChain=llmAndParserChained.invoke(messagesToTheLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab7326c1-4d5a-40e7-9ac0-f04c3baaad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.\n"
     ]
    }
   ],
   "source": [
    "print(responseFromChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4d263-909c-4bdb-b9f1-e512923c69e8",
   "metadata": {},
   "source": [
    "## Use a Prompt Template with a System Message and Variables\n",
    "* variables:\n",
    "    * text_input: the text the user wants to translate \n",
    "    * language: the language the user wants to translate the text_input into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61100547-85b6-4bb3-a17d-11a5de63bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ed49b3b-c0c1-49cd-b52a-a526563c6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following text input into {language}:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a12e297-5ead-4c23-b91a-cc1d1c1aaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template), \n",
    "        (\"user\", \"{text_input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "785aeccb-fed1-4522-8f67-0ed0a05f6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecondChain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12eb314d-e386-4c82-bee8-ed5eb06898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponseFromTheLLM = SecondChain.invoke(\n",
    "    {\n",
    "    \"language\": \"Spanish\",\n",
    "    \"text_input\": \"Now is the moment to learn Generative AI!\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cc0527f-17b8-4f8c-9b5a-37c5efd18ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Ahora es el momento de aprender la IA Generativa!\n"
     ]
    }
   ],
   "source": [
    "print(ResponseFromTheLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9826d1-5d80-4456-86a6-bfbf69db51c0",
   "metadata": {},
   "source": [
    "## Deploy the App using LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c9635ec-22b1-43e2-85f7-316b83380945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38089b3a-d4cb-49aa-9282-1aaec1a3c391",
   "metadata": {},
   "source": [
    "* See the file simpleTranslator.py in your code editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65a0f62a-6ddd-483a-b0d7-801b4bc4c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "\n",
    "# from fastapi import FastAPI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langserve import add_routes\n",
    "\n",
    "# import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "# openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     ('system', system_template),\n",
    "#     ('user', '{text}')\n",
    "# ])\n",
    "\n",
    "# chain = prompt_template | llm | parser\n",
    "\n",
    "# app = FastAPI(\n",
    "#   title=\"simpleTranslator\",\n",
    "#   version=\"1.0\",\n",
    "#   description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    "# )\n",
    "\n",
    "# add_routes(\n",
    "#     app,\n",
    "#     chain,\n",
    "#     path=\"/chain\",\n",
    "# )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "\n",
    "#     uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645a0cb-c045-4836-a619-63b76fcb09bf",
   "metadata": {},
   "source": [
    "#### Run the server\n",
    "* In terminal:\n",
    "    * python simpleTranslator.py\n",
    "* Check the app in the browser:\n",
    "    * [http://localhost:8000/](http://localhost:8000/)\n",
    "    * Nothing fancy there... yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003a928-db18-4c16-92e3-594180c4c5ae",
   "metadata": {},
   "source": [
    "#### Now let's try the app using the \"toy demo UI\" provided by LangServe Playground\n",
    "* Try the app in the browser:\n",
    "    * [http://localhost:8000/chain/playground/](http://localhost:8000/chain/playground/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc518dd-a621-4f64-850c-12632133e379",
   "metadata": {},
   "source": [
    "#### And finally, let's see how we can use the LangServe Playground from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a67a8b87-dadf-448c-a639-168914356140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La inteligencia artificial generativa es una oportunidad más grande que Internet.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "remote_chain.invoke({\"language\": \"Spanish\", \"text\": \"Generative AI is a bigger opportunity than Internet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b2972-99a4-4e33-960e-bf10b7ac469e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
